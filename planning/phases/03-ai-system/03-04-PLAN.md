---
phase: 03-ai-system
plan: 04
type: execute
wave: 3
depends_on: ["03-02", "03-03"]
files_modified:
  - convex/kapso.ts
autonomous: false

must_haves:
  truths:
    - "Incoming WhatsApp message triggers The Mouth for immediate response"
    - "After Mouth responds, The Brain analyzes conversation asynchronously"
    - "Lead score updates appear in contacts table"
    - "AI usage is logged to aiUsage table"
  artifacts:
    - path: "convex/kapso.ts"
      provides: "Updated processARI with Mouth+Brain orchestration"
      contains: "generateMouthResponse"
  key_links:
    - from: "convex/kapso.ts"
      to: "convex/ai/mouth.ts"
      via: "ctx.runAction"
      pattern: "runAction.*generateMouthResponse"
    - from: "convex/kapso.ts"
      to: "convex/ai/brain.ts"
      via: "ctx.scheduler.runAfter"
      pattern: "scheduler\\.runAfter.*analyzeConversation"
---

<objective>
Wire Mouth and Brain into processARI — complete the dual-AI orchestration

Purpose: Update the existing processARI function in kapso.ts to use the new Mouth and Brain modules. The Mouth responds immediately to users, then The Brain analyzes asynchronously.

Output: Updated kapso.ts with working dual-AI flow, verified end-to-end with a test WhatsApp message.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@planning/PROJECT.md
@planning/phases/03-ai-system/03-RESEARCH.md
@convex/kapso.ts
@convex/ai/mouth.ts
@convex/ai/brain.ts
@convex/ai/context.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update processARI to use Mouth and Brain modules</name>
  <files>convex/kapso.ts</files>
  <action>
Replace the existing generateAIResponse function and update processARI to use the new AI modules.

**CONTEXT: processARI already has a `messageHistory` variable** that loads recent messages from ariMessages table. This existing code looks like:

```typescript
// Get recent messages for context (existing code in processARI)
const recentMessages = await ctx.db
  .query("ariMessages")
  .withIndex("by_conversation", (q) => q.eq("conversation_id", ariConversation._id))
  .order("desc")
  .take(10);

const messageHistory = recentMessages.reverse();
```

You do NOT need to add new message loading code — use the existing `messageHistory` variable.

**Changes to convex/kapso.ts:**

1. Add imports at the top:
```typescript
import { internal } from "./_generated/api";
```

2. Update the processARI handler to:
   a. Call The Mouth for immediate response
   b. Schedule The Brain for async analysis
   c. Log Mouth usage to aiUsage table

Replace the generateAIResponse call in processARI with:

```typescript
// Inside processARI handler, after getting recent messages:
// NOTE: messageHistory is already loaded above from ariMessages query

// 1. THE MOUTH: Generate immediate response
const mouthResponse = await ctx.runAction(internal.ai.mouth.generateMouthResponse, {
  conversationHistory: messageHistory.map((m) => ({
    role: m.role,
    content: m.content,
  })),
  userMessage: user_message,
  botName: ariConfig.bot_name,
  contactName: contact?.name || contact?.kapso_name || undefined,
  language: ariConfig.language,
});

// 2. Log Mouth usage
if (mouthResponse.model !== "fallback") {
  await ctx.db.insert("aiUsage", {
    workspace_id: workspace_id as any,
    conversation_id: ariConversation._id,
    model: mouthResponse.model,
    ai_type: "mouth",
    input_tokens: mouthResponse.tokens,
    output_tokens: 0, // Mouth doesn't track output separately
    cost_usd: mouthResponse.model === "grok-beta"
      ? mouthResponse.tokens * (5 / 1_000_000) // Grok: ~$5/M tokens
      : 0, // Sea-Lion: free (local)
    created_at: Date.now(),
  });
}

// Use mouthResponse.content instead of aiResponse.content for the rest
const aiResponse = {
  content: mouthResponse.content,
  model: mouthResponse.model,
  tokens: mouthResponse.tokens,
  kapso_message_id: undefined as string | undefined,
};
```

3. After saving the AI response message, schedule The Brain:

```typescript
// 3. THE BRAIN: Schedule async analysis (after response sent)
// This runs after the user already received the Mouth's response
ctx.scheduler.runAfter(1000, internal.ai.brain.analyzeConversation, {
  workspaceId: workspace_id as any,
  contactId: contact_id as any,
  ariConversationId: ariConversation._id,
  recentMessages: [
    ...messageHistory.map((m) => ({
      role: m.role,
      content: m.content,
    })),
    { role: "user", content: user_message },
    { role: "assistant", content: mouthResponse.content },
  ],
  contactName: contact?.name || contact?.kapso_name || undefined,
  currentScore: (contact as any)?.lead_score || 0,
});

console.log(`[ARI] Scheduled Brain analysis for conversation`);
```

4. Remove or comment out the old generateAIResponse function and formatMessages helper (they're replaced by the Mouth module).

IMPORTANT:
- Use `ctx.runAction` to call The Mouth (it's an internalAction)
- Use `ctx.scheduler.runAfter` to schedule The Brain (async, 1s delay)
- The existing `messageHistory` variable from processARI contains the recent messages — do not create a new query
- Keep workspace_id and contact_id casts as needed for legacy code
- The Mouth responds first, Brain analyzes second
  </action>
  <verify>Run `npx convex dev` and confirm no type errors. Check console for successful deployment.</verify>
  <done>processARI uses generateMouthResponse for immediate response and schedules analyzeConversation for async lead scoring.</done>
</task>

<task type="auto">
  <name>Task 2: Add mouth usage tracking to kapso.ts</name>
  <files>convex/kapso.ts</files>
  <action>
Create a helper mutation in kapso.ts (or in ai/brain.ts if preferred) to log Mouth usage:

Add this mutation to kapso.ts:

```typescript
/**
 * Log Mouth AI usage (called inline since processARI is already a mutation).
 */
export const logMouthUsage = internalMutation({
  args: {
    workspaceId: v.string(),
    conversationId: v.string(),
    model: v.string(),
    tokens: v.number(),
  },
  handler: async (ctx, args) => {
    // Calculate cost: Grok = ~$5/M tokens, Sea-Lion = free
    const costUsd = args.model === "grok-beta"
      ? args.tokens * (5 / 1_000_000)
      : 0;

    await ctx.db.insert("aiUsage", {
      workspace_id: args.workspaceId as any,
      conversation_id: args.conversationId as any,
      model: args.model,
      ai_type: "mouth",
      input_tokens: args.tokens,
      output_tokens: 0,
      cost_usd: costUsd,
      created_at: Date.now(),
    });
  },
});
```

Alternative: Since processARI is already an internalMutation, you can insert directly into the aiUsage table without a separate mutation. The inline approach in Task 1 is preferred for simplicity.
  </action>
  <verify>Check that aiUsage inserts compile without type errors. Run `npx convex dev`.</verify>
  <done>Mouth usage is logged to aiUsage table with model attribution and cost calculation.</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 3: End-to-end verification with test WhatsApp message</name>
  <what-built>
Complete dual-AI orchestration:
- The Mouth generates immediate WhatsApp responses
- The Brain analyzes conversations asynchronously
- Usage is tracked in aiUsage table
- Lead scores update in contacts table
  </what-built>
  <how-to-verify>
1. Send a test WhatsApp message to the Eagle Overseas number:
   - Message: "Halo, saya mau tanya tentang kuliah di Australia"

2. Check Convex Dashboard (https://dashboard.convex.dev/t/intent-otter-212):
   - **ariMessages table**: New user message + assistant response appear
   - **aiUsage table**: At least 2 records:
     - One for Mouth (ai_type="mouth", model="sea-lion" or "grok-beta")
     - One for Brain (ai_type="brain", model="claude-haiku-4.5")
   - **contacts table**: lead_score updated from 0 to >0
   - **ariConversations table**: state updated (likely "greeting" or "qualifying")

3. Check console logs in Convex dashboard:
   - [ARI] Processing message...
   - [ARI] Response sent in Xms (sea-lion or grok-beta)
   - [ARI] Scheduled Brain analysis...
   - [Brain] Analysis complete: score=X, temp=X, cost=$X

4. Verify response time:
   - WhatsApp response should arrive within 3 seconds
  </how-to-verify>
  <resume-signal>Type "approved" if all verification steps pass, or describe any issues found.</resume-signal>
</task>

</tasks>

<verification>
1. Code: `npx convex dev` succeeds without errors
2. Integration: processARI calls Mouth, schedules Brain
3. Usage: aiUsage table has records for both mouth and brain
4. Scoring: contacts.lead_score updates after Brain analysis
5. E2E: WhatsApp message gets bot response within 3 seconds
</verification>

<success_criteria>
- processARI imports and calls internal.ai.mouth.generateMouthResponse
- processARI schedules internal.ai.brain.analyzeConversation with 1s delay
- aiUsage table receives records for both mouth and brain AI calls
- contacts.lead_score updates based on Brain analysis
- WhatsApp test message receives response and triggers both AI modules
</success_criteria>

<output>
After completion, create `planning/phases/03-ai-system/03-04-SUMMARY.md`
</output>
