---
phase: 03-ai-system
plan: 02
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - convex/ai/mouth.ts
  - convex/ai/context.ts
autonomous: true

user_setup:
  - service: grok
    why: "Fallback conversational AI when Sea-Lion unavailable"
    env_vars:
      - name: GROK_API_KEY
        source: "xAI Console -> API Keys (https://console.x.ai)"
    dashboard_config: []

must_haves:
  truths:
    - "The Mouth generates conversational responses via Sea-Lion or Grok fallback"
    - "Context builder formats conversation history for AI consumption"
    - "Response time target is under 2 seconds"
  artifacts:
    - path: "convex/ai/mouth.ts"
      provides: "Conversational AI module (The Mouth)"
      exports: ["generateMouthResponse"]
    - path: "convex/ai/context.ts"
      provides: "Context builder for AI calls"
      exports: ["buildConversationContext", "buildMouthSystemPrompt", "buildBrainSystemPrompt"]
  key_links:
    - from: "convex/ai/mouth.ts"
      to: "http://100.113.96.25:11434"
      via: "Ollama API call"
      pattern: "fetch.*11434"
    - from: "convex/ai/mouth.ts"
      to: "https://api.x.ai"
      via: "Grok fallback"
      pattern: "fetch.*api\\.x\\.ai"
---

<objective>
Create "The Mouth" — conversational AI module with Sea-Lion primary and Grok fallback

Purpose: The Mouth handles real-time WhatsApp conversations with low latency (<2s). It uses Sea-Lion (local Ollama on Tailscale) for free Indonesian/English responses, falling back to Grok API when unavailable.

Output: convex/ai/mouth.ts with generateMouthResponse, convex/ai/context.ts with prompt builders.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@planning/PROJECT.md
@planning/phases/03-ai-system/03-RESEARCH.md
@planning/phases/03-ai-system/03-01-SUMMARY.md
@convex/kapso.ts
@convex/schema.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create context builder module</name>
  <files>convex/ai/context.ts</files>
  <action>
Create convex/ai/context.ts with shared context utilities for both Mouth and Brain:

```typescript
/**
 * Context builders for AI modules.
 *
 * The Mouth uses short context (last 10 messages) for speed.
 * The Brain uses full context (last 20 messages) for analysis.
 */

export interface ConversationMessage {
  role: "user" | "assistant" | "system";
  content: string;
  metadata?: Record<string, unknown>;
}

export interface ContextOptions {
  maxMessages?: number;
  aiType: "mouth" | "brain";
}

/**
 * Build conversation context for AI consumption.
 * Mouth: last 10 messages (speed critical)
 * Brain: last 20 messages (analysis needs more context)
 */
export function buildConversationContext(
  messages: Array<{ role: string; content: string; metadata?: unknown }>,
  options: ContextOptions
): ConversationMessage[] {
  const { maxMessages, aiType } = options;

  // Different window sizes for different AI types
  const contextWindow = aiType === "mouth" ? 10 : 20;
  const limit = maxMessages ?? contextWindow;

  // Take most recent messages
  const recentMessages = messages.slice(-limit);

  return recentMessages.map((m) => ({
    role: m.role as "user" | "assistant" | "system",
    content: m.content,
    metadata: m.metadata as Record<string, unknown> | undefined,
  }));
}

/**
 * Build system prompt for The Mouth (conversational AI).
 * Style: Short, friendly, Indonesian-optimized.
 */
export function buildMouthSystemPrompt(
  botName: string = "Ari",
  contactName: string = "kakak",
  language: string = "id"
): string {
  const isIndonesian = language === "id";

  if (isIndonesian) {
    return `Kamu adalah ${botName}, asisten AI dari Eagle Overseas Education Indonesia.

Kamu sedang berbicara dengan ${contactName}.

Tugasmu:
1. Sapa pelanggan dengan ramah (sesuai waktu: pagi/siang/sore/malam)
2. Jawab pertanyaan singkat tentang studi ke luar negeri
3. Kumpulkan info dasar: nama lengkap, negara tujuan, level pendidikan
4. Tanya dokumen satu per satu (passport, CV, IELTS, ijazah)
5. Tawarkan konsultasi gratis atau komunitas setelah dapat info

Style:
- Singkat (1-2 kalimat max)
- Santai dan ramah, seperti teman
- JANGAN pakai emoji
- Tanya satu hal per pesan
- Mirror bahasa customer (ID/EN)
- Jika tidak tahu, bilang "Bentar ya, saya tanyakan dulu ke tim"

Jawab dengan cepat dan langsung ke inti.`;
  }

  return `You are ${botName}, an AI assistant from Eagle Overseas Education.

You're speaking with ${contactName}.

Your tasks:
1. Greet warmly (match time of day)
2. Answer brief questions about studying abroad
3. Collect basic info: full name, destination country, education level
4. Ask about documents one by one (passport, CV, IELTS, transcript)
5. Offer free consultation or community after getting info

Style:
- Brief (1-2 sentences max)
- Friendly and casual, like a friend
- NO emojis
- Ask one thing per message
- If unsure, say "Let me check with the team"

Respond quickly and directly.`;
}

/**
 * Build system prompt for The Brain (analytical AI).
 * Style: Structured, analytical, decision-focused.
 */
export function buildBrainSystemPrompt(): string {
  return `You are an AI lead scoring analyst for Eagle Overseas Education.

Your task: Analyze WhatsApp conversations to:

1. Score leads (0-100) based on:
   - Basic info collected (name, email, destination): 25 points
   - Qualification signals (budget, timeline, documents ready): 35 points
   - Engagement level (response speed, question quality): 10 points
   - Document readiness (passport, CV, IELTS, transcript): 30 points

2. Classify lead temperature:
   - HOT (70+): Has budget, timeline, documents ready
   - WARM (40-69): Interested but missing documents or timeline unclear
   - COLD (<40): Just browsing, no commitment signals

3. Determine conversation state:
   - greeting: Initial contact, getting basic info
   - qualifying: Collecting documents, assessing readiness
   - scheduling: Ready for consultation booking
   - handoff: Needs human (pricing, complaints, complex questions)

4. Recommend next action:
   - continue_bot: Bot can handle this
   - offer_consultation: Lead is hot, offer 1-on-1
   - offer_community: Warm lead, invite to free community
   - handoff_human: Requires human intervention

RESPOND ONLY WITH VALID JSON:
{
  "lead_score": <0-100>,
  "temperature": "<hot|warm|cold>",
  "state": "<greeting|qualifying|scheduling|handoff>",
  "next_action": "<continue_bot|offer_consultation|offer_community|handoff_human>",
  "reasoning": "<brief explanation>"
}`;
}
```

IMPORTANT: Use TypeScript types, no `any`. Export all functions for use by mouth.ts and brain.ts.
  </action>
  <verify>File exists at convex/ai/context.ts. Run `npx tsc --noEmit convex/ai/context.ts` to check types.</verify>
  <done>convex/ai/context.ts exports buildConversationContext, buildMouthSystemPrompt, buildBrainSystemPrompt with proper types.</done>
</task>

<task type="auto">
  <name>Task 2: Create The Mouth module</name>
  <files>convex/ai/mouth.ts</files>
  <action>
Create convex/ai/mouth.ts with generateMouthResponse function:

```typescript
/**
 * The Mouth — Conversational AI module.
 *
 * Handles real-time WhatsApp conversations with low latency.
 * Primary: Sea-Lion (local Ollama on Tailscale) — FREE
 * Fallback: Grok API — PAID
 */

import { internalAction } from "../_generated/server";
import { v } from "convex/values";
import {
  buildConversationContext,
  buildMouthSystemPrompt,
  type ConversationMessage
} from "./context";

export interface MouthResponse {
  content: string;
  model: "sea-lion" | "grok-beta" | "fallback";
  tokens: number;
  responseTimeMs: number;
}

/**
 * Generate a conversational response from The Mouth.
 *
 * Flow:
 * 1. Try Sea-Lion (local Ollama) first — free, Indonesian-optimized
 * 2. If Sea-Lion fails, fall back to Grok API — paid but reliable
 * 3. If both fail, return safe fallback message
 */
export const generateMouthResponse = internalAction({
  args: {
    conversationHistory: v.array(v.object({
      role: v.string(),
      content: v.string(),
    })),
    userMessage: v.string(),
    botName: v.optional(v.string()),
    contactName: v.optional(v.string()),
    language: v.optional(v.string()),
  },
  handler: async (ctx, args): Promise<MouthResponse> => {
    const startTime = Date.now();

    // Build context with sliding window (last 10 messages for speed)
    const context = buildConversationContext(
      args.conversationHistory,
      { aiType: "mouth", maxMessages: 10 }
    );

    // Build system prompt
    const systemPrompt = buildMouthSystemPrompt(
      args.botName ?? "Ari",
      args.contactName ?? "kakak",
      args.language ?? "id"
    );

    // Format messages for API
    const messages: ConversationMessage[] = [
      { role: "system", content: systemPrompt },
      ...context,
      { role: "user", content: args.userMessage },
    ];

    // Try Sea-Lion first (free, local)
    try {
      const seaLionResponse = await callSeaLion(messages);
      if (seaLionResponse) {
        return {
          ...seaLionResponse,
          responseTimeMs: Date.now() - startTime,
        };
      }
    } catch (error) {
      console.error("[Mouth] Sea-Lion error, falling back to Grok:", error);
    }

    // Fall back to Grok
    try {
      const grokResponse = await callGrok(messages);
      if (grokResponse) {
        return {
          ...grokResponse,
          responseTimeMs: Date.now() - startTime,
        };
      }
    } catch (error) {
      console.error("[Mouth] Grok error:", error);
    }

    // Final fallback
    return {
      content: args.language === "en"
        ? "Thank you for contacting us. Our consultant will help you shortly."
        : "Terima kasih sudah menghubungi kami. Konsultan kami akan segera membantu.",
      model: "fallback",
      tokens: 0,
      responseTimeMs: Date.now() - startTime,
    };
  },
});

/**
 * Call Sea-Lion via local Ollama (Tailscale server).
 * FREE - runs locally on 100.113.96.25:11434
 */
async function callSeaLion(
  messages: ConversationMessage[]
): Promise<Omit<MouthResponse, "responseTimeMs"> | null> {
  const ollamaUrl = process.env.SEALION_URL || "http://100.113.96.25:11434";

  // Format messages for Ollama chat API
  const formattedMessages = messages.map((m) => ({
    role: m.role,
    content: m.content,
  }));

  const response = await fetch(`${ollamaUrl}/api/chat`, {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({
      model: "sea-lion",  // or "aisingapore/Gemma-SEA-LION-v3-9B-IT"
      messages: formattedMessages,
      stream: false,
      options: {
        num_ctx: 2048,
        temperature: 0.8,
        top_p: 0.9,
      },
    }),
  });

  if (!response.ok) {
    console.error(`[Mouth] Ollama error: ${response.status}`);
    return null;
  }

  const data = await response.json();
  const content = data.message?.content || data.response || "";

  if (!content) {
    return null;
  }

  return {
    content,
    model: "sea-lion",
    tokens: data.eval_count || 0,
  };
}

/**
 * Call Grok via xAI API.
 * PAID fallback when Sea-Lion unavailable.
 */
async function callGrok(
  messages: ConversationMessage[]
): Promise<Omit<MouthResponse, "responseTimeMs"> | null> {
  const grokApiKey = process.env.GROK_API_KEY;

  if (!grokApiKey) {
    console.warn("[Mouth] No GROK_API_KEY configured");
    return null;
  }

  // Format for OpenAI-compatible API
  const formattedMessages = messages.map((m) => ({
    role: m.role,
    content: m.content,
  }));

  const response = await fetch("https://api.x.ai/v1/chat/completions", {
    method: "POST",
    headers: {
      Authorization: `Bearer ${grokApiKey}`,
      "Content-Type": "application/json",
    },
    body: JSON.stringify({
      model: "grok-beta",
      messages: formattedMessages,
      max_tokens: 150,  // Keep responses short
      temperature: 0.8,
    }),
  });

  if (!response.ok) {
    const errorText = await response.text();
    console.error(`[Mouth] Grok error: ${response.status} - ${errorText}`);
    return null;
  }

  const data = await response.json();
  const content = data.choices?.[0]?.message?.content || "";

  if (!content) {
    return null;
  }

  return {
    content,
    model: "grok-beta",
    tokens: data.usage?.total_tokens || 0,
  };
}
```

IMPORTANT:
- Use `internalAction` (not internalMutation) because it makes external API calls
- No `any` types — use explicit interfaces
- Include console.error for debugging in production
  </action>
  <verify>Run `npx convex dev` and confirm no type errors. Check convex/ai/mouth.ts compiles.</verify>
  <done>convex/ai/mouth.ts exports generateMouthResponse internalAction that calls Sea-Lion with Grok fallback.</done>
</task>

</tasks>

<verification>
1. Files: convex/ai/context.ts and convex/ai/mouth.ts exist
2. Types: `npx convex dev` succeeds without type errors
3. Exports: Both files have documented exports
</verification>

<success_criteria>
- convex/ai/context.ts exports buildConversationContext, buildMouthSystemPrompt, buildBrainSystemPrompt
- convex/ai/mouth.ts exports generateMouthResponse internalAction
- Sea-Lion call targets http://100.113.96.25:11434
- Grok fallback uses https://api.x.ai/v1/chat/completions
- No TypeScript `any` types in new files
</success_criteria>

<output>
After completion, create `planning/phases/03-ai-system/03-02-SUMMARY.md`
</output>
