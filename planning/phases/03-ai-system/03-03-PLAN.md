---
phase: 03-ai-system
plan: 03
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - convex/ai/brain.ts
autonomous: true

user_setup:
  - service: anthropic
    why: "Claude API for analytical AI (The Brain)"
    env_vars:
      - name: ANTHROPIC_API_KEY
        source: "Anthropic Console -> API Keys (https://console.anthropic.com/settings/keys)"
    dashboard_config: []

must_haves:
  truths:
    - "The Brain analyzes conversations and scores leads"
    - "Lead score updates are written to contacts and ariConversations"
    - "Analysis uses Claude Haiku for cost optimization"
  artifacts:
    - path: "convex/ai/brain.ts"
      provides: "Analytical AI module (The Brain)"
      exports: ["analyzeConversation"]
  key_links:
    - from: "convex/ai/brain.ts"
      to: "https://api.anthropic.com"
      via: "Anthropic SDK"
      pattern: "anthropic\\.messages\\.create"
    - from: "convex/ai/brain.ts"
      to: "convex/schema.ts"
      via: "aiUsage insert"
      pattern: "ctx\\.runMutation.*aiUsage"
---

<objective>
Create "The Brain" — analytical AI module for lead scoring and conversation analysis

Purpose: The Brain runs asynchronously after The Mouth responds, analyzing conversations to score leads, update CRM data, and decide when to handoff to humans. Uses Claude Haiku for cost-optimized analysis ($1/$5 per million tokens).

Output: convex/ai/brain.ts with analyzeConversation function that updates lead scores and conversation state.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@planning/PROJECT.md
@planning/phases/03-ai-system/03-RESEARCH.md
@planning/phases/03-ai-system/03-01-SUMMARY.md
@convex/schema.ts
@convex/ai/context.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create The Brain module</name>
  <files>convex/ai/brain.ts</files>
  <action>
Create convex/ai/brain.ts with analyzeConversation function:

```typescript
/**
 * The Brain — Analytical AI module.
 *
 * Runs asynchronously after The Mouth responds.
 * Analyzes conversations to:
 * - Score leads (0-100)
 * - Classify temperature (hot/warm/cold)
 * - Determine conversation state
 * - Recommend next action
 *
 * Uses Claude Haiku for cost-optimized analysis ($1/$5 per million tokens).
 */

import Anthropic from "@anthropic-ai/sdk";
import { internalAction } from "../_generated/server";
import { internal } from "../_generated/api";
import { v } from "convex/values";
import { buildConversationContext, buildBrainSystemPrompt } from "./context";

export interface BrainAnalysis {
  lead_score: number;
  temperature: "hot" | "warm" | "cold";
  state: "greeting" | "qualifying" | "scheduling" | "handoff";
  next_action: "continue_bot" | "offer_consultation" | "offer_community" | "handoff_human";
  reasoning: string;
}

export interface BrainResponse {
  analysis: BrainAnalysis;
  model: string;
  inputTokens: number;
  outputTokens: number;
  cacheReadTokens: number;
  costUsd: number;
}

/**
 * Analyze a conversation and update lead scores.
 *
 * This action:
 * 1. Loads recent conversation history
 * 2. Calls Claude Haiku for analysis
 * 3. Updates contact lead_score and lead_status
 * 4. Updates ariConversation state and temperature
 * 5. Logs usage to aiUsage table
 */
export const analyzeConversation = internalAction({
  args: {
    workspaceId: v.id("workspaces"),
    contactId: v.id("contacts"),
    ariConversationId: v.id("ariConversations"),
    recentMessages: v.array(v.object({
      role: v.string(),
      content: v.string(),
    })),
    contactName: v.optional(v.string()),
    currentScore: v.optional(v.number()),
  },
  handler: async (ctx, args): Promise<BrainResponse | null> => {
    const anthropicKey = process.env.ANTHROPIC_API_KEY;

    if (!anthropicKey) {
      console.error("[Brain] No ANTHROPIC_API_KEY configured");
      return null;
    }

    // Build context with full history (last 20 messages)
    const context = buildConversationContext(
      args.recentMessages,
      { aiType: "brain", maxMessages: 20 }
    );

    // Build analysis prompt
    const systemPrompt = buildBrainSystemPrompt();
    const analysisPrompt = buildAnalysisPrompt(
      context,
      args.contactName ?? "Unknown",
      args.currentScore ?? 0
    );

    try {
      // Call Claude Haiku with prompt caching
      const anthropic = new Anthropic({ apiKey: anthropicKey });

      const response = await anthropic.messages.create({
        model: "claude-haiku-4-5-20250929",
        max_tokens: 500,
        system: [
          {
            type: "text",
            text: systemPrompt,
            // Enable prompt caching for system prompt
            cache_control: { type: "ephemeral" },
          },
        ],
        messages: [
          { role: "user", content: analysisPrompt },
        ],
      });

      // Extract text response
      const textContent = response.content.find((c) => c.type === "text");
      if (!textContent || textContent.type !== "text") {
        console.error("[Brain] No text content in response");
        return null;
      }

      // Parse JSON response
      let analysis: BrainAnalysis;
      try {
        analysis = JSON.parse(textContent.text);
      } catch (parseError) {
        console.error("[Brain] Failed to parse JSON:", textContent.text);
        // Return default analysis on parse failure
        analysis = {
          lead_score: args.currentScore ?? 0,
          temperature: "cold",
          state: "greeting",
          next_action: "continue_bot",
          reasoning: "Parse error - using defaults",
        };
      }

      // Calculate cost
      const inputTokens = response.usage.input_tokens;
      const outputTokens = response.usage.output_tokens;
      const cacheReadTokens = (response.usage as Record<string, number>).cache_read_input_tokens ?? 0;
      const costUsd = calculateClaudeCost(inputTokens, outputTokens, cacheReadTokens);

      // Update contact lead score
      await ctx.runMutation(internal.ai.brain.updateContactScore, {
        contactId: args.contactId,
        leadScore: analysis.lead_score,
        leadStatus: mapTemperatureToStatus(analysis.temperature),
      });

      // Update ariConversation state
      await ctx.runMutation(internal.ai.brain.updateConversationState, {
        ariConversationId: args.ariConversationId,
        state: analysis.state,
        leadScore: analysis.lead_score,
        leadTemperature: analysis.temperature,
      });

      // Log usage to aiUsage table
      await ctx.runMutation(internal.ai.brain.logBrainUsage, {
        workspaceId: args.workspaceId,
        conversationId: args.ariConversationId,
        model: "claude-haiku-4.5",
        inputTokens,
        outputTokens,
        cacheReadTokens,
        costUsd,
      });

      console.log(
        `[Brain] Analysis complete: score=${analysis.lead_score}, temp=${analysis.temperature}, cost=$${costUsd.toFixed(6)}`
      );

      return {
        analysis,
        model: "claude-haiku-4.5",
        inputTokens,
        outputTokens,
        cacheReadTokens,
        costUsd,
      };
    } catch (error) {
      console.error("[Brain] Analysis failed:", error);
      return null;
    }
  },
});

/**
 * Build the analysis prompt with conversation context.
 */
function buildAnalysisPrompt(
  messages: Array<{ role: string; content: string }>,
  contactName: string,
  currentScore: number
): string {
  const conversationText = messages
    .map((m) => `${m.role === "user" ? "Customer" : "Bot"}: ${m.content}`)
    .join("\n");

  return `Analyze this conversation and provide lead scoring.

Contact Info:
- Name: ${contactName}
- Current Score: ${currentScore}

Recent Conversation:
${conversationText}

Provide your analysis as JSON.`;
}

/**
 * Calculate Claude API cost.
 * Haiku pricing: $1 per million input, $5 per million output
 * Cache reads: 90% discount on input
 */
function calculateClaudeCost(
  inputTokens: number,
  outputTokens: number,
  cacheReadTokens: number
): number {
  const INPUT_COST_PER_MILLION = 1.0;  // $1 per million
  const OUTPUT_COST_PER_MILLION = 5.0; // $5 per million
  const CACHE_DISCOUNT = 0.9;          // 90% discount for cached

  const regularInputCost = (inputTokens - cacheReadTokens) * (INPUT_COST_PER_MILLION / 1_000_000);
  const cachedInputCost = cacheReadTokens * (INPUT_COST_PER_MILLION * (1 - CACHE_DISCOUNT) / 1_000_000);
  const outputCost = outputTokens * (OUTPUT_COST_PER_MILLION / 1_000_000);

  return regularInputCost + cachedInputCost + outputCost;
}

/**
 * Map temperature to contact lead_status field.
 */
function mapTemperatureToStatus(temperature: "hot" | "warm" | "cold"): string {
  switch (temperature) {
    case "hot": return "hot";
    case "warm": return "warm";
    case "cold": return "cold";
    default: return "new";
  }
}
```

This is the action file. Now add the helper mutations in the same file (after the action):

```typescript
// ============================================
// HELPER MUTATIONS (called by analyzeConversation)
// ============================================

import { internalMutation } from "../_generated/server";

/**
 * Update contact lead score and status.
 */
export const updateContactScore = internalMutation({
  args: {
    contactId: v.id("contacts"),
    leadScore: v.number(),
    leadStatus: v.string(),
  },
  handler: async (ctx, args) => {
    await ctx.db.patch(args.contactId, {
      lead_score: args.leadScore,
      lead_status: args.leadStatus,
      updated_at: Date.now(),
    });
  },
});

/**
 * Update ariConversation state and scoring.
 */
export const updateConversationState = internalMutation({
  args: {
    ariConversationId: v.id("ariConversations"),
    state: v.string(),
    leadScore: v.number(),
    leadTemperature: v.string(),
  },
  handler: async (ctx, args) => {
    await ctx.db.patch(args.ariConversationId, {
      state: args.state,
      lead_score: args.leadScore,
      lead_temperature: args.leadTemperature,
      updated_at: Date.now(),
    });
  },
});

/**
 * Log AI usage to aiUsage table.
 */
export const logBrainUsage = internalMutation({
  args: {
    workspaceId: v.id("workspaces"),
    conversationId: v.id("ariConversations"),
    model: v.string(),
    inputTokens: v.number(),
    outputTokens: v.number(),
    cacheReadTokens: v.number(),
    costUsd: v.number(),
  },
  handler: async (ctx, args) => {
    await ctx.db.insert("aiUsage", {
      workspace_id: args.workspaceId,
      conversation_id: args.conversationId,
      model: args.model,
      ai_type: "brain",
      input_tokens: args.inputTokens,
      output_tokens: args.outputTokens,
      cache_read_tokens: args.cacheReadTokens,
      cost_usd: args.costUsd,
      created_at: Date.now(),
    });
  },
});
```

IMPORTANT:
- Use `internalAction` for the main function (makes external API calls)
- Use `internalMutation` for database updates (called via ctx.runMutation)
- Import from `internal` API (not `api`) for internal functions
- Cast cache_read_input_tokens properly (Anthropic SDK types may not include it)
  </action>
  <verify>Run `npx convex dev` and confirm no type errors. Check that analyzeConversation, updateContactScore, updateConversationState, logBrainUsage are all exported.</verify>
  <done>convex/ai/brain.ts exports analyzeConversation internalAction with helper mutations for updating contacts, conversations, and logging usage.</done>
</task>

<task type="auto">
  <name>Task 2: Create cost-tracker query module</name>
  <files>convex/ai/cost-tracker.ts</files>
  <action>
Create convex/ai/cost-tracker.ts with queries for cost analysis:

```typescript
/**
 * Cost Tracker — Usage monitoring for AI modules.
 *
 * Provides queries for:
 * - Total costs by workspace
 * - Cost breakdown by AI type (mouth vs brain)
 * - Per-conversation costs
 */

import { query } from "../_generated/server";
import { v } from "convex/values";

export interface CostSummary {
  mouth: {
    cost: number;
    conversations: number;
    totalTokens: number;
  };
  brain: {
    cost: number;
    conversations: number;
    totalTokens: number;
    cacheHitRate: number;
  };
  total: number;
}

/**
 * Get cost summary for a workspace in a date range.
 */
export const getWorkspaceCosts = query({
  args: {
    workspaceId: v.id("workspaces"),
    fromTimestamp: v.number(),
    toTimestamp: v.number(),
  },
  handler: async (ctx, args): Promise<CostSummary> => {
    const usageRecords = await ctx.db
      .query("aiUsage")
      .withIndex("by_workspace", (q) => q.eq("workspace_id", args.workspaceId))
      .collect();

    // Filter by date range in memory (Convex doesn't support range on non-indexed fields)
    const filtered = usageRecords.filter(
      (r) => r.created_at >= args.fromTimestamp && r.created_at <= args.toTimestamp
    );

    // Aggregate by AI type
    const mouthRecords = filtered.filter((r) => r.ai_type === "mouth");
    const brainRecords = filtered.filter((r) => r.ai_type === "brain");

    const mouthCost = mouthRecords.reduce((sum, r) => sum + r.cost_usd, 0);
    const brainCost = brainRecords.reduce((sum, r) => sum + r.cost_usd, 0);

    const mouthTokens = mouthRecords.reduce(
      (sum, r) => sum + r.input_tokens + r.output_tokens,
      0
    );
    const brainTokens = brainRecords.reduce(
      (sum, r) => sum + r.input_tokens + r.output_tokens,
      0
    );

    // Calculate cache hit rate for Brain
    const totalBrainInput = brainRecords.reduce((sum, r) => sum + r.input_tokens, 0);
    const totalCacheRead = brainRecords.reduce(
      (sum, r) => sum + (r.cache_read_tokens ?? 0),
      0
    );
    const cacheHitRate = totalBrainInput > 0 ? totalCacheRead / totalBrainInput : 0;

    // Count unique conversations
    const mouthConvIds = new Set(
      mouthRecords.map((r) => r.conversation_id).filter(Boolean)
    );
    const brainConvIds = new Set(
      brainRecords.map((r) => r.conversation_id).filter(Boolean)
    );

    return {
      mouth: {
        cost: mouthCost,
        conversations: mouthConvIds.size,
        totalTokens: mouthTokens,
      },
      brain: {
        cost: brainCost,
        conversations: brainConvIds.size,
        totalTokens: brainTokens,
        cacheHitRate,
      },
      total: mouthCost + brainCost,
    };
  },
});

/**
 * Get cost for a specific conversation.
 */
export const getConversationCost = query({
  args: {
    conversationId: v.id("ariConversations"),
  },
  handler: async (ctx, args) => {
    const usageRecords = await ctx.db
      .query("aiUsage")
      .withIndex("by_conversation", (q) => q.eq("conversation_id", args.conversationId))
      .collect();

    const totalCost = usageRecords.reduce((sum, r) => sum + r.cost_usd, 0);
    const mouthCost = usageRecords
      .filter((r) => r.ai_type === "mouth")
      .reduce((sum, r) => sum + r.cost_usd, 0);
    const brainCost = usageRecords
      .filter((r) => r.ai_type === "brain")
      .reduce((sum, r) => sum + r.cost_usd, 0);

    return {
      total: totalCost,
      mouth: mouthCost,
      brain: brainCost,
      callCount: usageRecords.length,
    };
  },
});

/**
 * Get recent AI calls for debugging.
 */
export const getRecentCalls = query({
  args: {
    workspaceId: v.id("workspaces"),
    limit: v.optional(v.number()),
  },
  handler: async (ctx, args) => {
    const limit = args.limit ?? 20;

    const records = await ctx.db
      .query("aiUsage")
      .withIndex("by_workspace", (q) => q.eq("workspace_id", args.workspaceId))
      .order("desc")
      .take(limit);

    return records.map((r) => ({
      id: r._id,
      model: r.model,
      aiType: r.ai_type,
      inputTokens: r.input_tokens,
      outputTokens: r.output_tokens,
      costUsd: r.cost_usd,
      createdAt: r.created_at,
    }));
  },
});
```

IMPORTANT: Use `withIndex` for queries, not `filter` callbacks. Filter by date range in memory after the query.
  </action>
  <verify>Run `npx convex dev` and confirm no type errors. Check that getWorkspaceCosts, getConversationCost, getRecentCalls are exported.</verify>
  <done>convex/ai/cost-tracker.ts exports queries for workspace costs, conversation costs, and recent AI calls.</done>
</task>

</tasks>

<verification>
1. Files: convex/ai/brain.ts and convex/ai/cost-tracker.ts exist
2. Types: `npx convex dev` succeeds without type errors
3. Exports: brain.ts has analyzeConversation, cost-tracker.ts has getWorkspaceCosts
</verification>

<success_criteria>
- convex/ai/brain.ts exports analyzeConversation internalAction
- convex/ai/brain.ts exports updateContactScore, updateConversationState, logBrainUsage mutations
- convex/ai/cost-tracker.ts exports getWorkspaceCosts, getConversationCost, getRecentCalls queries
- Claude Haiku model ID is "claude-haiku-4-5-20250929" (current as of 2026)
- Prompt caching enabled via cache_control: { type: "ephemeral" }
</success_criteria>

<output>
After completion, create `planning/phases/03-ai-system/03-03-SUMMARY.md`
</output>
