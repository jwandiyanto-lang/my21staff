---
phase: 02-ari-core-conversation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/lib/ari/types.ts
  - src/lib/ari/clients/grok.ts
  - src/lib/ari/clients/sealion.ts
  - src/lib/ari/ai-router.ts
  - src/lib/ari/index.ts
  - package.json
autonomous: true

must_haves:
  truths:
    - "OpenAI SDK is installed and configured"
    - "Grok API client connects successfully"
    - "Sea-Lion API client connects to Ollama"
    - "A/B model selection is deterministic per contact"
  artifacts:
    - path: "src/lib/ari/types.ts"
      provides: "ARI TypeScript interfaces"
      contains: "ARIState"
    - path: "src/lib/ari/ai-router.ts"
      provides: "Multi-LLM routing"
      exports: ["selectModel", "createAIClient"]
  key_links:
    - from: "src/lib/ari/ai-router.ts"
      to: "openai SDK"
      via: "import OpenAI"
      pattern: "import OpenAI from"
---

# 02-01-PLAN: ARI Foundation - Types and AI Clients

## Goal

Set up ARI TypeScript types and multi-LLM clients (Grok + Sea-Lion) with deterministic A/B routing.

## Context

**Research findings (02-RESEARCH.md):**
- Use OpenAI SDK for both Grok and Sea-Lion (both support OpenAI-compatible API)
- Grok: baseURL `https://api.x.ai/v1`, env `GROK_API_KEY`
- Sea-Lion: baseURL `http://100.113.96.25:11434/v1` (Ollama via Tailscale)
- Deterministic model selection based on contact_id hash (prevents A/B test contamination)

**Database schema (34_ari_tables.sql):**
- ari_config: bot_name, greeting_style, language, tone, community_link
- ari_conversations: state, lead_score, lead_temperature, context, ai_model
- ari_messages: role, content, ai_model, tokens_used, response_time_ms

## Tasks

<task id="1">
<title>Install OpenAI SDK and create ARI types</title>
<action>
1. Install openai SDK:
   ```bash
   npm install openai
   ```

2. Create `src/lib/ari/types.ts` with:
   - ARIState type: 'greeting' | 'qualifying' | 'scoring' | 'booking' | 'payment' | 'scheduling' | 'handoff' | 'completed'
   - ARIContext interface for JSONB context field (collected data, form data, score breakdown)
   - ARIConfig interface matching ari_config table
   - ARIConversation interface matching ari_conversations table
   - ARIMessage interface matching ari_messages table
   - STATE_TRANSITIONS map defining valid state transitions
   - AIModelType: 'grok' | 'sealion'
   - AIResponse interface with content, tokens, responseTimeMs

3. Use exact field names from database schema. Include JSDoc comments explaining each state.
</action>
<verify>
- File exists at src/lib/ari/types.ts
- TypeScript compiles without errors: `npx tsc --noEmit src/lib/ari/types.ts`
- ARIState includes all 8 states from schema
</verify>
</task>

<task id="2">
<title>Create Grok and Sea-Lion AI clients</title>
<action>
1. Create `src/lib/ari/clients/grok.ts`:
   - Import OpenAI from 'openai'
   - Create grokClient with baseURL 'https://api.x.ai/v1' and GROK_API_KEY
   - Export generateGrokResponse(messages, options) function
   - Track response time, return AIResponse with tokens_used
   - Default model: 'grok-3' (faster), allow override to 'grok-4'
   - max_tokens: 150 (WhatsApp brevity), temperature: 0.8

2. Create `src/lib/ari/clients/sealion.ts`:
   - Import OpenAI from 'openai'
   - Create sealionClient with baseURL from env SEALION_URL or default 'http://100.113.96.25:11434/v1'
   - apiKey: 'ollama' (Ollama doesn't need real key)
   - Export generateSealionResponse(messages, options) function
   - Model: 'aisingapore/Gemma-SEA-LION-v4-27B-IT'
   - Same AIResponse return format

3. Both clients should:
   - Accept OpenAI.ChatCompletionMessageParam[] as messages
   - Handle errors gracefully, return fallback message on failure
   - Log timing info for debugging
</action>
<verify>
- Files exist at src/lib/ari/clients/grok.ts and src/lib/ari/clients/sealion.ts
- No TypeScript errors
- Both export generate*Response functions
</verify>
</task>

<task id="3">
<title>Create AI router with deterministic A/B selection</title>
<action>
1. Create `src/lib/ari/ai-router.ts`:
   - Import both client modules
   - selectModel(contactId: string, grokWeight?: number): AIModelType
     - Hash contactId to get deterministic 0-99 value
     - If hash < grokWeight (default 50), return 'grok', else 'sealion'
     - This ensures same contact always gets same model
   - generateResponse(model: AIModelType, messages, options): Promise<AIResponse>
     - Route to appropriate client based on model

2. Create `src/lib/ari/index.ts`:
   - Re-export all types from types.ts
   - Re-export selectModel, generateResponse from ai-router.ts
   - Re-export individual clients for direct access if needed

3. Hash function should use string charCode sum modulo 100 (simple, deterministic):
   ```typescript
   const hash = contactId.split('').reduce((acc, char) =>
     ((acc << 5) - acc) + char.charCodeAt(0), 0
   );
   return Math.abs(hash) % 100;
   ```
</action>
<verify>
- src/lib/ari/index.ts exports: ARIState, selectModel, generateResponse
- selectModel returns consistent results for same contactId
- Manual test: same contactId returns same model on multiple calls
</verify>
</task>

## Verification

- [ ] `npm install` completes without errors
- [ ] All files in src/lib/ari/ exist and compile
- [ ] Types match database schema (state values, field names)
- [ ] A/B selection is deterministic (test with fixed contactId)

## Success Criteria

- OpenAI SDK installed in package.json
- ARI types defined matching database schema
- Grok client configured with x.ai endpoint
- Sea-Lion client configured with Ollama endpoint
- Deterministic A/B model selection working

## Requirements Addressed

- **ARI-07** (partial): Configurable AI model selection for persona/tone
